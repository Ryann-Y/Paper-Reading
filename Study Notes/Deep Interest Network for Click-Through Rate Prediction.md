**论文地址**：[https://arxiv.org/pdf/1706.06978](https://arxiv.org/pdf/1706.06978)
**源码地址**：[https://github.com/zhougr1993/DeepInterestNetwork](https://github.com/zhougr1993/DeepInterestNetwork)

**参考**：
- https://zhuanlan.zhihu.com/p/51623339
- https://zhuanlan.zhihu.com/p/408502932

**问题背景**：
1. 在CTR预估任务重，传统的 MLP 模型架构中有限维度的用户表征向量对于表达用户复杂的兴趣来说是极大的瓶颈；
2. 为了提升用户向量的表征，可能会采用增大用户表征维度的方式，但这会极大增加参数量，且带来过拟合的风险。同时还会增加计算的负担；
3. 传统的 L2 正则化在每个min-batch计算时都需要全部参数的参与，计算量很大。

**名词解释**：
- DIN: **D**eep **I**nterest **N**etwork, 深度兴趣网络
- MBA: **M**ini-**B**atch **A**ware regularization, 小批量感知正则化

**论文贡献**：
1. 提出了一种全新的模型结构 DIN，可以通过考虑候选商品和用户行为之间的相关性动态计算用户兴趣的向量表达。通过引入局部激活单元，DIN 集中通过软搜索相关的用户行为并采用加权求和 pooling 的方式获得用户表征向量；
2. 针对L2正则化提出了一种 min-batch aware regularization 的方法，让只有在每个 min-batch 中出现的非零特征参与L2的计算，降低了整体的计算量；
3. 设计了一种新的数据自适应激活函数 **Dice**，该方法通过考虑输入分布对PReLU进行了泛化，具有良好的性能。


**模型结构**：
![[Pasted image 20240524152730.png|800]]

**mini-batch aware regularization** 是一种用于大规模稀疏场景下的正则化方法，它可以减少计算开销和过拟合的风险。它的主要思想是只对每个mini-batch中参数不为0的部分进行梯度更新，而不是对整个参数矩阵进行更新。这样可以避免对那些没有出现在mini-batch中的特征进行惩罚，从而保留更多的信息。此外，它还在一定程度上解决了数据长尾分布的过拟合问题。对长尾部分样本施加较大的惩罚而对短尾部分施加较小的惩罚来防止模型对于长尾部分的过拟合。

具体来说，假设有一个特征矩阵 $X$，它是一个高维稀疏的二进制向量，每一行代表一个样本，每一列代表一个特征。我们希望将这个特征矩阵映射到一个低维稠密的向量，即进行嵌入（embedding）操作。我们可以定义一个嵌入矩阵 $W$，它的每一列代表一个特征的嵌入向量。如果我们使用L2正则化来防止过拟合，那么我们需要对 $W$ 进行惩罚，即在损失函数中加入一个正则项： $L=L_{0}+\lambda \Vert W\Vert^{2}$ ，那么我们需要对 $W$ 的每一个元素进行更新。

这样做有两个问题：
- 一是计算开销过大。因为 $X$ 是一个高维稀疏的矩阵，所以 $W$ 也是一个高维稀疏的矩阵。如果我们对 $W$ 的每一个元素都进行更新，那么我们需要遍历整个 $W$ 矩阵，这会消耗大量的时间和空间。
- 二是信息丢失。因为X是一个稀疏的矩阵，所以有很多特征在某些样本中并没有出现。如果我们对这些特征对应的嵌入向量也进行惩罚，那么我们就会降低这些特征的权重，从而丢失了这些特征所包含的信息。

为了解决这两个问题，mini-batch aware regularization提出了一个近似的方法。它的基本思想是只对每个mini-batch中参数不为0的部分进行梯度更新，而不是对整个参数矩阵进行更新。具体来说，假设我们将训练集分成了若干个mini-batch，每个mini-batch包含了m个样本。那么对于第 $k$ 个mini-batch，我们可以定义一个指示矩阵 $I_{k}$，它的大小和 $W$ 相同，从而只对batch中参数不为 0 的部分进行梯度更新。具体来说，如果第k个mini-batch中的第 $i$ 个样本的第 $j$ 个特征为1，那么 $I_{k}$ 的第 $j$ 列为1，否则为0。那么，我们可以用 $I_{k}$ 来过滤 $W$，得到一个只包含第 $k$ 个mini-batch中出现过的特征的嵌入矩阵 $W_{k}$：

$W_{k}=W\odot I_{k}$

其中， $\odot$ 是按位乘法（Hadamard product）。

这样，我们就可以只对 $W_{k}$ 进行L2正则化，而不是对整个 $W$ 进行正则化。这样可以**减少计算开销**，也可以**保留更多的信息**。我们可以将[损失函数](https://www.zhihu.com/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2976101537%7D)改写为：

$L=L_{0}+\lambda\Vert W_{k}\Vert^{2}$

这样，我们就实现了mini-batch aware regularization的方法。

mini-batch aware regularization 的缺点是它可能会引入一些偏差，因为它只考虑了mini-batch中的数据分布，而不是整个数据集的分布。这样可能会**导致一些特征被过分地放大或缩小，从而影响模型的泛化能力**。另一个缺点是它可能会**增加方差**，因为它会使得每个mini-batch中的参数更新不一致，从而导致模型的不稳定。

![[Pasted image 20240524164011.png|600]]

![[Pasted image 20240524162437.png|400]]


>思考：当 DIN 的序列过长的时候，比如超过100个，会出现信息淹没的情况，即使 attention 机制也没办法学习到很好