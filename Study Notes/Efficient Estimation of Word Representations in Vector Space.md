**论文地址**：[https://arxiv.org/pdf/1301.3781](https://arxiv.org/pdf/1301.3781)
**参考**：
- https://zhuanlan.zhihu.com/p/53194407

文章提出了两个最新的用于从大数据集计算 **连续词向量表示** 的模型结构，向量表示的评估在一个词相似任务上进行。结果显示，词语表征的相似性超越了简单的句法规律
。例如，向量("King")-向量("Man")+向量("Woman")得到的向量最接近单词Queen的向量表示.

**名词解释**：
- NNLM：**N**eural **N**etwork **L**anguage **M**odel，神经网络语言模型
- CBOW：**C**ontinuous **B**age-**o**f-**W**ords Model，连续词袋模型
- Continuous Skip-gram Model，连续跳格模型

**问题背景**：
- 在目前的 NLP 系统和技术当中，通常将 词 作为一个原子单元，由于通常将词表示为 词表中的下标，因此单词之间没有相似的概念；这种方式有如下好处：
	- 简单性、健壮性、用大量数据训练的简单模型优于用较少数据训练的复杂系统。
- 但这种方式同样存在一定局限性。

**模型结构**：
![[Pasted image 20240531115316.png|600]]

**CBOW**：
在 PROJECTION 阶段，所有单词 共享映射位置，向量被平均。由于单词之间的顺序不影响映射关系，因此称为词袋模型。
>all words get projected into the same position (their vectors are averaged)

论文中采用了 4个 历史单词和4个未来单词作为输入，预测分类中间词，训练复杂度为：$Q=N\times D+D\times \log_{2}(V)$.

**Skip-gram**：
以中间词为输入，预测前后词。

>由于距离较远的单词与当前单词的相关性通常低于与其相近的单词，因此我们通过在训练示例中减少对这些单词的采样来减少对距离较远的单词的权重。

**策略收益**：
- 极低的计算成本带来非常大的准确率提升